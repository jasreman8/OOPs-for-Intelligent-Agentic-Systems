{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasreman8/OOPs-for-Intelligent-Agentic-Systems/blob/main/Asynchronous_Programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f829bc",
      "metadata": {
        "id": "a3f829bc"
      },
      "source": [
        "# Learning Objectives\n",
        "\n",
        "*   Understand the basic concepts of asynchronous programming in Python using `async` and `await`.\n",
        "*   Recognize why asynchronous programming is beneficial when dealing with I/O-bound operations (like network requests to LLMs).\n",
        "*   Learn how to define and call asynchronous functions.\n",
        "*   See how LangChain components, particularly LLMs like `ChatOpenAI`, provide asynchronous methods (e.g., `.ainvoke()`, `.astream()`, `.abatch()`).\n",
        "*   Understand the use of `async with` for managing resources asynchronously (though less directly used with simple LLM calls, it's a core async concept).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ae0f2e",
      "metadata": {
        "id": "b0ae0f2e"
      },
      "source": [
        "# `async` and `await`: The Keywords for Asynchronous Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86005a0a",
      "metadata": {
        "id": "86005a0a"
      },
      "source": [
        "So far, most of the Python code we've written has been **synchronous**. This means when you call a function, your program waits for that function to complete before moving on to the next line of code. If that function involves waiting (e.g., for a network request to an LLM to come back), your whole program just sits there, waiting.\n",
        "\n",
        "**Asynchronous programming** allows your program to do other useful work while it's waiting for slow operations to finish. It doesn't necessarily make the slow operation itself faster, but it makes your overall application more efficient and responsive because it's not \"blocked.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01535633",
      "metadata": {
        "id": "01535633"
      },
      "source": [
        "Python provides the `async` and `await` keywords to write asynchronous code. This is often used with the `asyncio` library.\n",
        "\n",
        "*   **`async def`**: Used to define an **asynchronous function** (also called a **coroutine**).\n",
        "    *   When you call an `async def` function, it doesn't run immediately. Instead, it returns a **coroutine object**.\n",
        "*   **`await`**: Used *inside* an `async def` function to pause its execution until an \"awaitable\" operation (like another coroutine or certain I/O operations) completes.\n",
        "    *   While `await` is waiting, Python's event loop can switch to run other tasks, making the program non-blocking.\n",
        "*   **Event Loop (`asyncio.run()`)**: To actually run an `async def` function from synchronous code and manage the execution of asynchronous tasks, you typically use an event loop. `asyncio.run(coroutine_object)` is a common way to start it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddcfa580",
      "metadata": {
        "id": "ddcfa580"
      },
      "source": [
        "**Simple Conceptual Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93a759b1",
      "metadata": {
        "id": "93a759b1"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import time\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply() # required only in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synchronous instance"
      ],
      "metadata": {
        "id": "xsmgPCsM3R6Q"
      },
      "id": "xsmgPCsM3R6Q"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tea(tea_type: str, delay: int) -> str:\n",
        "    print(f\"Starting to make {tea_type} tea... (will take {delay}s)\")\n",
        "    time.sleep(delay) # Simulate a time-consuming I/O operation (like steeping)\n",
        "    print(f\"{tea_type} tea is ready!\")\n",
        "    return f\"{tea_type} tea\""
      ],
      "metadata": {
        "id": "WSUZjEpv1J6X"
      },
      "id": "WSUZjEpv1J6X",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def toast_bread(slices: int, toast_type: str, delay: int) -> str:\n",
        "    print(f\"Starting to toast {slices} slice(s) of {toast_type} bread... (will take {delay}s)\")\n",
        "    time.sleep(delay) # Simulate toasting time\n",
        "    print(f\"Bread is toasted!\")\n",
        "    return f\"{slices} toasted slice(s)\""
      ],
      "metadata": {
        "id": "FFTIVG2T1KCJ"
      },
      "id": "FFTIVG2T1KCJ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_breakfast():\n",
        "    print(\"--- Starting breakfast preparation (asynchronously) ---\\n\")\n",
        "\n",
        "    # Start both tasks (coroutines) concurrently\n",
        "    # asyncio.gather runs multiple awaitables concurrently and waits for all to complete.\n",
        "    tea_task = make_tea(\"Masala\", 3)\n",
        "    toast_task = toast_bread(2, 'french omlette', 2)\n",
        "\n",
        "    # Await their results\n",
        "    # The 'await' here means 'make_breakfast' will pause until BOTH tea_task AND toast_task are done.\n",
        "    # But tea_task and toast_task can run \"at the same time\" (interleaved) by the event loop.\n",
        "    results = (tea_task, toast_task)\n",
        "\n",
        "    tea_result, toast_result = results # Unpack results\n",
        "\n",
        "    print(f\"\\n--- Breakfast is served! ---\")\n",
        "    print(f\"- {tea_result}\")\n",
        "    print(f\"- {toast_result}\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "hFJ_m7Bv1KF1"
      },
      "id": "hFJ_m7Bv1KF1",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# It manages the event loop.\n",
        "breakfast_items = make_breakfast()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal breakfast preparation time: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBWhfW_Y1KIs",
        "outputId": "6db44064-e5ce-4e2c-c24d-0b71104af186"
      },
      "id": "mBWhfW_Y1KIs",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting breakfast preparation (asynchronously) ---\n",
            "\n",
            "Starting to make Masala tea... (will take 3s)\n",
            "Masala tea is ready!\n",
            "Starting to toast 2 slice(s) of french omlette bread... (will take 2s)\n",
            "Bread is toasted!\n",
            "\n",
            "--- Breakfast is served! ---\n",
            "- Masala tea\n",
            "- 2 toasted slice(s)\n",
            "\n",
            "Total breakfast preparation time: 5.00 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aysnchronous Instance"
      ],
      "metadata": {
        "id": "VYHZONDt3utO"
      },
      "id": "VYHZONDt3utO"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "828a7838",
      "metadata": {
        "id": "828a7838"
      },
      "outputs": [],
      "source": [
        "async def make_tea(tea_type: str, delay: int) -> str:\n",
        "    print(f\"Starting to make {tea_type} tea... (will take {delay}s)\")\n",
        "    await asyncio.sleep(delay) # Simulate a time-consuming I/O operation (like steeping)\n",
        "    print(f\"{tea_type} tea is ready!\")\n",
        "    return f\"{tea_type} tea\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "79df9b58",
      "metadata": {
        "id": "79df9b58"
      },
      "outputs": [],
      "source": [
        "async def toast_bread(slices: int, toast_type: str, delay: int) -> str:\n",
        "    print(f\"Starting to toast {slices} slice(s) of {toast_type} bread... (will take {delay}s)\")\n",
        "    await asyncio.sleep(delay) # Simulate toasting time\n",
        "    print(f\"Bread is toasted!\")\n",
        "    return f\"{slices} toasted slice(s)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fb9afb2f",
      "metadata": {
        "id": "fb9afb2f"
      },
      "outputs": [],
      "source": [
        "async def make_breakfast():\n",
        "    print(\"--- Starting breakfast preparation (asynchronously) ---\")\n",
        "\n",
        "    # Start both tasks (coroutines) concurrently\n",
        "    # asyncio.gather runs multiple awaitables concurrently and waits for all to complete.\n",
        "    tea_task = make_tea(\"Masala\", 3)\n",
        "    toast_task = toast_bread(2, 'french omlette', 2)\n",
        "\n",
        "    # Await their results\n",
        "    # The 'await' here means 'make_breakfast' will pause until BOTH tea_task AND toast_task are done.\n",
        "    # But tea_task and toast_task can run \"at the same time\" (interleaved) by the event loop.\n",
        "    results = await asyncio.gather(tea_task, toast_task)\n",
        "\n",
        "    tea_result, toast_result = results # Unpack results\n",
        "\n",
        "    print(f\"\\n--- Breakfast is served! ---\")\n",
        "    print(f\"- {tea_result}\")\n",
        "    print(f\"- {toast_result}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e672d389",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e672d389",
        "outputId": "953638e4-9f2b-4c35-e62b-6ad053cfbe00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting breakfast preparation (asynchronously) ---\n",
            "Starting to make Masala tea... (will take 3s)\n",
            "Starting to toast 2 slice(s) of french omlette bread... (will take 2s)\n",
            "Bread is toasted!\n",
            "Masala tea is ready!\n",
            "\n",
            "--- Breakfast is served! ---\n",
            "- Masala tea\n",
            "- 2 toasted slice(s)\n",
            "\n",
            "Total breakfast preparation time: 3.01 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# asyncio.run() takes a coroutine and runs it until completion.\n",
        "# It manages the event loop.\n",
        "breakfast_items = asyncio.run(make_breakfast())\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal breakfast preparation time: {end_time - start_time:.2f} seconds\")\n",
        "# This time will be close to the LONGER of the two tasks (3s for tea),\n",
        "# not the sum (3s + 2s = 5s), because they ran concurrently"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b504629f",
      "metadata": {
        "id": "b504629f"
      },
      "source": [
        "**Why is this useful?**\n",
        "\n",
        "Imagine `make_tea` and `toast_bread` were calls to different slow web services. With synchronous code, you'd wait for tea, *then* start toasting. With `async`/`await`, you can effectively initiate both requests, and the program can work on other things (or just efficiently wait) until the responses come back.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3d6471",
      "metadata": {
        "id": "4c3d6471"
      },
      "source": [
        "This asynchronous paradigm is very useful when working with LLMs as well. Calling an LLM is a network I/O operation. It can take several seconds. If you need to make multiple LLM calls (e.g., to summarize different documents, or ask multiple questions), doing them asynchronously can significantly speed up your overall application."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f0d9cd",
      "metadata": {
        "id": "65f0d9cd"
      },
      "source": [
        "As an example, let us look at how asynchronous programmming works with LLM calls in LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59ba874",
      "metadata": {
        "id": "c59ba874"
      },
      "source": [
        "# LangChain's Asynchronous Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "764d2d85",
      "metadata": {
        "id": "764d2d85"
      },
      "source": [
        "LangChain `Runnable` components (including LLMs, Prompts, Parsers, Chains) are designed with asynchronous operations in mind. They typically provide asynchronous versions of their main execution methods:\n",
        "\n",
        "*   `.invoke()` (synchronous)  ->  `.ainvoke()` (asynchronous)\n",
        "*   `.stream()` (synchronous)  ->  `.astream()` (asynchronous)\n",
        "*   `.batch()` (synchronous)   ->  `.abatch()` (asynchronous)\n",
        "\n",
        "These `a` prefixed methods are `async def` methods themselves and need to be `await`ed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langchain-openai==0.3.24"
      ],
      "metadata": {
        "id": "sWMec3RY-I4h"
      },
      "id": "sWMec3RY-I4h",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "50f57f29",
      "metadata": {
        "id": "50f57f29"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import time\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "nest_asyncio.apply() # required only in notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e2977bc8",
      "metadata": {
        "id": "e2977bc8"
      },
      "outputs": [],
      "source": [
        "# Initialize LLM (same as before)\n",
        "llm = ChatOpenAI(\n",
        "    api_key=userdata.get('OPEN_API_KEY'),\n",
        "    base_url=\"https://aibe.mygreatlearning.com/openai/v1\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "399173a0",
      "metadata": {
        "id": "399173a0"
      },
      "outputs": [],
      "source": [
        "# --- Define a simple chain (also a Runnable) ---\n",
        "prompt = ChatPromptTemplate.from_template(\"Write a very short, positive quote about {topic}.\")\n",
        "parser = StrOutputParser()\n",
        "quote_chain = prompt | llm | parser # This chain is also a Runnable and has .ainvoke()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e324a5d3",
      "metadata": {
        "id": "e324a5d3"
      },
      "outputs": [],
      "source": [
        "# --- Asynchronous function to get a single quote ---\n",
        "async def get_single_quote_async(topic: str) -> str:\n",
        "    print(f\"Async: Requesting quote for '{topic}'...\")\n",
        "    # .ainvoke() is an async method, so we await it\n",
        "    result = await quote_chain.ainvoke({\"topic\": topic}) #over here we are awaiting the .ainvoke even if it is used for the first time.\n",
        "    print(f\"Async: Received quote for '{topic}': '{result[:30]}...'\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "70084345",
      "metadata": {
        "id": "70084345"
      },
      "outputs": [],
      "source": [
        "# --- Asynchronous function to get multiple quotes concurrently using .abatch() on the chain ---\n",
        "async def get_multiple_quotes_concurrently_chain_abatch(topics: list[str]) -> list[str]:\n",
        "    print(f\"\\nAsync Batch (Chain): Requesting quotes for topics: {topics} using chain.abatch()...\")\n",
        "    # Create input dictionaries for each topic\n",
        "    batch_inputs = [{\"topic\": topic} for topic in topics]\n",
        "    # .abatch() on a chain takes a list of inputs and processes them concurrently\n",
        "    results = await quote_chain.abatch(batch_inputs) #over here we are awaiting the .abatch even if it is used for the first time.\n",
        "    print(\"Async Batch (Chain): All quotes received.\")\n",
        "    for i, topic in enumerate(topics):\n",
        "        print(f\"  - Topic '{topic}': '{results[i][:30]}...'\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dfe6ea03",
      "metadata": {
        "id": "dfe6ea03"
      },
      "outputs": [],
      "source": [
        "# --- Asynchronous function to get multiple quotes concurrently using asyncio.gather ---\n",
        "# This demonstrates making multiple independent .ainvoke calls concurrently\n",
        "async def get_multiple_quotes_concurrently_gather(topics: list[str]) -> list[str]:\n",
        "    print(f\"\\nAsync Gather: Requesting quotes for topics: {topics} using asyncio.gather with chain.ainvoke()...\")\n",
        "\n",
        "    # Create a list of coroutine objects (tasks)\n",
        "    tasks = [quote_chain.ainvoke({\"topic\": topic}) for topic in topics]\n",
        "\n",
        "    # asyncio.gather runs all tasks concurrently\n",
        "    results = await asyncio.gather(*tasks) # Use * to unpack the list of tasks\n",
        "    #Finally await finds its way in the async gather function.\n",
        "\n",
        "    print(\"Async Gather: All quotes received.\")\n",
        "    for i, topic in enumerate(topics):\n",
        "        print(f\"  - Topic '{topic}': '{results[i][:30]}...'\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_all(topics):\n",
        "    tasks = [quote_chain.ainvoke({\"topic\": t}) for t in topics]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    print(\"Async Gather: All trial quotes received.\")\n",
        "    for i, topic in enumerate(topics):\n",
        "        print(f\"  - Topic '{topic}': '{results[i][:65]}...'\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "xesaSWNNkOI6"
      },
      "id": "xesaSWNNkOI6",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f6b58942",
      "metadata": {
        "id": "f6b58942"
      },
      "outputs": [],
      "source": [
        "# --- Main async execution function ---\n",
        "async def main_async_operations():\n",
        "    start_time_main = time.time()\n",
        "\n",
        "    # 1. Get a single quote asynchronously\n",
        "    single_quote_task = get_single_quote_async(\"perseverance\")\n",
        "\n",
        "    # 2. Get multiple quotes using chain.abatch()\n",
        "    topics_for_batch = [\"innovation\", \"teamwork\", \"learning\"]\n",
        "    batch_quotes_task = get_multiple_quotes_concurrently_chain_abatch(topics_for_batch)\n",
        "\n",
        "    # 3. Get multiple quotes using asyncio.gather with chain.ainvoke()\n",
        "    topics_for_gather = [\"creativity\", \"leadership\"] # Different topics to see separate calls\n",
        "    gather_quotes_task = get_multiple_quotes_concurrently_gather(topics_for_gather)\n",
        "\n",
        "    # 4. Get multiple quotes using asyncio.gather with chain.ainvoke()\n",
        "    topics_for_gather_2 = [\"motivation\", \"discipline\"] # Different topics to see separate calls\n",
        "    gather_run_task = run_all(topics_for_gather_2)\n",
        "\n",
        "    # Run all top-level tasks concurrently\n",
        "    # The individual print statements within the functions will show interleaving\n",
        "    await asyncio.gather(\n",
        "        single_quote_task,\n",
        "        batch_quotes_task,\n",
        "        gather_quotes_task,\n",
        "        gather_run_task\n",
        "    )\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal time for all async operations: {end_time_main - start_time_main:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "59f6a2e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59f6a2e4",
        "outputId": "57504279-874a-4493-d614-5eef77851d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SYNC Call (for baseline) ---\n",
            "Sync: Received quote for 'patience': '\"Patience is the gentle art of waiting for th...'\n",
            "Time for one sync call: 1.43s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Run the main async function ---\n",
        "# For comparison, let's do one synchronous call first\n",
        "print(\"--- SYNC Call (for baseline) ---\")\n",
        "sync_start_time = time.time()\n",
        "sync_result = quote_chain.invoke({\"topic\": \"patience\"})\n",
        "sync_end_time = time.time()\n",
        "print(f\"Sync: Received quote for 'patience': '{sync_result[:45]}...'\")\n",
        "print(f\"Time for one sync call: {sync_end_time - sync_start_time:.2f}s\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "496dbdec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "496dbdec",
        "outputId": "e8f976e6-8a91-468c-8eeb-b7a723e8eb24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ASYNC Operations ---\n",
            "Async: Requesting quote for 'perseverance'...\n",
            "\n",
            "Async Batch (Chain): Requesting quotes for topics: ['innovation', 'teamwork', 'learning'] using chain.abatch()...\n",
            "\n",
            "Async Gather: Requesting quotes for topics: ['creativity', 'leadership'] using asyncio.gather with chain.ainvoke()...\n",
            "Async Gather: All trial quotes received.\n",
            "  - Topic 'motivation': '\"Each step forward ignites the spark of possibility.\"...'\n",
            "  - Topic 'discipline': '\"Discipline is the bridge between goals and achievement.\"...'\n",
            "Async Batch (Chain): All quotes received.\n",
            "  - Topic 'innovation': '\"Innovation is the spark that ...'\n",
            "  - Topic 'teamwork': '\"Together we achieve more; uni...'\n",
            "  - Topic 'learning': '\"Learning is the key that unlo...'\n",
            "Async Gather: All quotes received.\n",
            "  - Topic 'creativity': '\"Creativity is the spark that ...'\n",
            "  - Topic 'leadership': '\"True leadership inspires othe...'\n",
            "Async: Received quote for 'perseverance': '\"Perseverance transforms obsta...'\n",
            "\n",
            "Total time for all async operations: 1.62 seconds\n"
          ]
        }
      ],
      "source": [
        "print(\"--- ASYNC Operations ---\")\n",
        "asyncio.run(main_async_operations())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c94272",
      "metadata": {
        "id": "79c94272"
      },
      "source": [
        "**Observations from the example:**\n",
        "*   When `asyncio.run(main_async_operations())` is called, the event loop starts.\n",
        "*   `get_single_quote_async`, `get_multiple_quotes_concurrently_chain_abatch`, and `get_multiple_quotes_concurrently_gather` are called. They return coroutine objects.\n",
        "*   `await asyncio.gather(...)` in `main_async_operations` allows these top-level tasks to proceed concurrently.\n",
        "*   Inside `get_multiple_quotes_concurrently_gather`, `await asyncio.gather(*tasks)` allows all the individual `quote_chain.ainvoke()` calls to OpenAI to happen \"in parallel\" from the perspective of your program (Python sends off the requests and can do other work or efficiently wait).\n",
        "*   Similarly, `quote_chain.abatch(batch_inputs)` is designed to make multiple requests to the LLM backend as efficiently as possible, often concurrently.\n",
        "*   The total time for all async operations will be significantly less than if you had made all those LLM calls synchronously one after another. It will be closer to the time taken by the slowest *set* of concurrent operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain-openai import ChatOpenAI\n",
        "import asyncio\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=userdata.get('OPEN_API_KEY'),\n",
        "    base_url=\"https://aibe.mygreatlearning.com/openai/v1\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7\n",
        ")\n",
        "prompt = PromptTemplate.from_template(\"What is {thing}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "def main():\n",
        "    result = chain.invoke({\"thing\":\"quantum computing\"})\n",
        "    print(result)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "NJm4BAKyHNON",
        "outputId": "103dec43-dd4e-4fa0-9519-b3fef3ca6893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "id": "NJm4BAKyHNON",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3552215880.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3552215880.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    from langchain-openai import ChatOpenAI\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJmBBAKyHNON",
        "outputId": "c2fdf8fd-b2bb-4c0a-b9d5-bbe0c8951c45"
      },
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "import asyncio\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=userdata.get('OPEN_API_KEY'),\n",
        "    base_url=\"https://aibe.mygreatlearning.com/openai/v1\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7\n",
        ")\n",
        "prompt = PromptTemplate.from_template(\"What is {thing}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "def main():\n",
        "    result = chain.invoke({\"thing\":\"quantum computing\"})\n",
        "    print(result)\n",
        "\n",
        "main()"
      ],
      "id": "NJmBBAKyHNON",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'thing': 'quantum computing', 'text': \"Quantum computing is a type of computing that harnesses the principles of quantum mechanics to process information in fundamentally different ways compared to classical computing. Here are some key concepts and features of quantum computing:\\n\\n1. **Qubits**: Unlike classical bits, which can be either 0 or 1, quantum bits (qubits) can exist in a superposition of states, meaning they can be both 0 and 1 simultaneously. This property enables quantum computers to process a vast amount of information in parallel.\\n\\n2. **Superposition**: This quantum phenomenon allows qubits to represent multiple combinations of states at once. When a qubit is in superposition, it can perform many calculations simultaneously, which can lead to significant speedups for certain types of problems.\\n\\n3. **Entanglement**: Qubits can become entangled, which means the state of one qubit is directly related to the state of another, no matter the distance between them. This property can be used to perform complex operations more efficiently than classical computers.\\n\\n4. **Quantum Gates**: Quantum computations are performed using quantum gates, which manipulate qubits through operations similar to logic gates in classical computing. These gates are reversible and can create complex quantum circuits.\\n\\n5. **Quantum Algorithms**: Some algorithms, such as Shor's algorithm for factoring large numbers and Grover's algorithm for searching unsorted databases, demonstrate that quantum computers can solve certain problems much faster than their classical counterparts.\\n\\n6. **Applications**: Quantum computing has potential applications in various fields, including cryptography, optimization, drug discovery, materials science, and complex system simulations.\\n\\n7. **Challenges**: Building practical quantum computers is still a significant challenge due to issues like qubit coherence, error rates, and scalability. Researchers are exploring various physical implementations of qubits, such as superconducting circuits, trapped ions, and topological qubits.\\n\\nOverall, quantum computing is a rapidly evolving field that promises to revolutionize computing by solving problems that are currently intractable for classical computers.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    api_key=userdata.get('OPEN_API_KEY'),\n",
        "    base_url=\"https://aibe.mygreatlearning.com/openai/v1\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7\n",
        ")\n",
        "prompt = PromptTemplate.from_template(\"What is {thing}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "async def main():\n",
        "    result = await chain.arun(thing=\"quantum computing\")\n",
        "    print(result)\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "id": "W8xkgEV1HOSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154efdc8-87c7-44cc-c353-db9acb4c449c"
      },
      "id": "W8xkgEV1HOSL",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantum computing is a type of computation that leverages the principles of quantum mechanics to process information in fundamentally different ways than classical computers. While classical computers use bits as the smallest unit of data, which can be either 0 or 1, quantum computers use quantum bits, or qubits. Qubits can exist in multiple states simultaneously due to a property known as superposition.\n",
            "\n",
            "Here are some key concepts associated with quantum computing:\n",
            "\n",
            "1. **Superposition**: Unlike classical bits, which are in one state at a time, qubits can exist in a combination of states. This allows quantum computers to process a vast amount of possibilities simultaneously.\n",
            "\n",
            "2. **Entanglement**: Qubits can become entangled, meaning the state of one qubit is directly related to the state of another, regardless of the distance between them. This property can be harnessed to perform complex calculations more efficiently.\n",
            "\n",
            "3. **Quantum Gates**: Quantum operations are performed using quantum gates, which manipulate qubits through mathematical operations. These gates form the basis of quantum algorithms, similar to how classical logic gates operate on bits.\n",
            "\n",
            "4. **Quantum Algorithms**: Some algorithms, like Shor's algorithm for factoring large integers and Grover's algorithm for searching unsorted databases, demonstrate the potential of quantum computing to solve certain problems significantly faster than classical computers.\n",
            "\n",
            "5. **Decoherence**: One of the challenges in quantum computing is maintaining the coherence of qubits. Environmental factors can cause qubits to lose their quantum state, which can lead to errors in computation.\n",
            "\n",
            "Quantum computing has the potential to revolutionize fields such as cryptography, optimization, drug discovery, and materials science by solving problems that are currently intractable for classical computers. However, practical quantum computers are still in the early stages of development, and significant challenges remain in terms of scalability, error correction, and qubit stability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Af-HDdVAZIZd"
      },
      "id": "Af-HDdVAZIZd",
      "execution_count": 36,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agentic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
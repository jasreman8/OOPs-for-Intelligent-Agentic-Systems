{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasreman8/OOPs-for-Intelligent-Agentic-Systems/blob/main/Polymorphism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f1e39f",
      "metadata": {
        "id": "19f1e39f"
      },
      "source": [
        "# Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ef8926",
      "metadata": {
        "id": "e5ef8926"
      },
      "source": [
        "- Understand the concept of polymorphism in Object-Oriented Programming.\n",
        "- Be introduced to the idea of an \"interface\" (or \"protocol\") as a contract for behavior.\n",
        "- See how LangChain's Runnable serves as a core interface.\n",
        "- Appreciate how polymorphism enables different components (LLMs, Prompts, Parsers, Retrievers) to be used interchangeably within LangChain Expression Language (LCEL) chains, as long as they adhere to the Runnable interface."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c89538",
      "metadata": {
        "id": "62c89538"
      },
      "source": [
        "# Introduction to Interfaces"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f2d73d8",
      "metadata": {
        "id": "3f2d73d8"
      },
      "source": [
        "We've learned about inheritance, where a child class gets features from a parent. Polymorphism (from Greek meaning \"many forms\") is a principle that often goes hand-in-hand with inheritance, but it's really about objects of different classes being treatable through a common interface.\n",
        "\n",
        "Imagine you have different types of payment methods: Credit Card, PayPal, Bank Transfer. You, as the merchant, just want to call a process_payment(amount) method. You don't want to write separate code for each payment type if the core action is the same. Polymorphism allows this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95919a6f",
      "metadata": {
        "id": "95919a6f"
      },
      "source": [
        "In Python, an \"interface\" isn't a strict keyword like in some other languages (e.g., Java's interface). Instead, it's more of a contract or a protocol. It defines a set of methods that a class promises to implement.\n",
        "\n",
        "If a class implements all the methods defined by an interface, it's said to \"adhere to\" or \"satisfy\" that interface.\n",
        "This means you can expect objects of that class to respond to those specific method calls.\n",
        "\n",
        "**Duck Typing (Python's Approach):**\n",
        "\n",
        "Python often uses \"duck typing\": \"If it walks like a duck and quacks like a duck, then it must be a duck.\"\n",
        "If an object has the methods you need (e.g., an `.invoke()` method), you can often use it as if it adheres to an expected interface, regardless of its specific class or inheritance hierarchy.\n",
        "\n",
        "**LangChain's `Runnable` as a Core Interface:**\n",
        "\n",
        "In LangChain, the `langchain_core.runnables.Runnable` class (and its associated protocols) acts as a fundamental interface. Most components designed to be part of an LCEL chain (Prompts, LLMs, Output Parsers, Retrievers, custom functions wrapped with RunnableLambda, etc.) adhere to the Runnable interface.\n",
        "\n",
        "This \"contract\" means they are expected to have certain methods, most notably:\n",
        "- `.invoke(input, config=None)`: Execute the Runnable with a single input.\n",
        "- `.stream(input, config=None)`: Stream the output for a single input.\n",
        "- `.batch(inputs, config=None)`: Execute with a batch of inputs.\n",
        "\n",
        "You don't usually create an object of `Runnable` itself. Instead, classes like `ChatPromptTemplate`, `ChatOpenAI`, `StrOutputParser` implement the `Runnable` interface (often by inheriting from base classes that themselves implement `Runnable`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0231adb4",
      "metadata": {
        "id": "0231adb4"
      },
      "source": [
        "# Polymorphism in Action: The `Runnable` Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b09a33",
      "metadata": {
        "id": "00b09a33"
      },
      "source": [
        "As we have seen, any component that is a `Runnable` promises to implement a set of common methods. Let us look more closely at the most important one for basic execution - `.invoke(input, config=None)`: This method is the primary way to execute a `Runnable` with a single input and get a single output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d56198b",
      "metadata": {
        "id": "9d56198b"
      },
      "source": [
        "Let's look at how different LangChain components, all being Runnables, implement .invoke() in their own specialized way."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langchain-openai==0.3.24"
      ],
      "metadata": {
        "id": "1oSdDjtq9apC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387a53e3-1726-44d1-8800-8d9a0dcddb0b"
      },
      "id": "1oSdDjtq9apC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fd20cbd3",
      "metadata": {
        "id": "fd20cbd3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dc970f19",
      "metadata": {
        "id": "dc970f19"
      },
      "outputs": [],
      "source": [
        "# --- Initialize our components ---\n",
        "# Each of these is a 'Runnable' and thus has an .invoke() method\n",
        "\n",
        "# 1. ChatPromptTemplate: A Runnable\n",
        "prompt_runnable = ChatPromptTemplate.from_template(\n",
        "    \"Generate a professional subject line for an email about {topic}.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e0db8547",
      "metadata": {
        "id": "e0db8547"
      },
      "outputs": [],
      "source": [
        "# 2. ChatOpenAI (LLM): A Runnable\n",
        "llm_runnable = ChatOpenAI(\n",
        "    api_key=userdata.get('OPEN_API_KEY'),\n",
        "    base_url=\"https://aibe.mygreatlearning.com/openai/v1\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3dd328f8",
      "metadata": {
        "id": "3dd328f8"
      },
      "outputs": [],
      "source": [
        "# 3. StrOutputParser: A Runnable\n",
        "parser_runnable = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e5be413e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5be413e",
        "outputId": "8c9f1b97-088e-4fc1-b7db-38193614df5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to prompt: {'topic': 'a new product launch for eco-friendly packaging'}\n",
            "\n",
            "Output of prompt.invoke() (type: <class 'langchain_core.prompt_values.ChatPromptValue'>):\n",
            "  - (human) Generate a professional subject line for an email about a new product launch for eco-friendly packaging.\n"
          ]
        }
      ],
      "source": [
        "# --- Demonstrating .invoke() on each Runnable component ---\n",
        "\n",
        "# A. Invoking the ChatPromptTemplate\n",
        "\n",
        "prompt_input = {\"topic\": \"a new product launch for eco-friendly packaging\"}\n",
        "formatted_prompt = prompt_runnable.invoke(prompt_input)\n",
        "\n",
        "print(f\"Input to prompt: {prompt_input}\")\n",
        "print(f\"\\nOutput of prompt.invoke() (type: {type(formatted_prompt)}):\")\n",
        "# For ChatPromptTemplate, the result is usually a ChatPromptValue, whose .to_messages() gives list of BaseMessage\n",
        "for message in formatted_prompt.to_messages():\n",
        "    print(f\"  - ({message.type}) {message.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb883db9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb883db9",
        "outputId": "ff180d06-1ed6-4b84-c10f-ba3608531e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. Invoking ChatOpenAI (LLM) ---\n",
            "\n",
            "Input to LLM (type: <class 'list'>): A list of messages\n",
            "\n",
            "Output of llm.invoke() (type: <class 'langchain_core.messages.ai.AIMessage'>):\n",
            "  - (ai) Content: '\"Introducing Our Innovative Eco-Friendly Packaging Solutions: Join the Green Revolution!\"'\n"
          ]
        }
      ],
      "source": [
        "# B. Invoking the ChatOpenAI LLM\n",
        "formatted_prompt = prompt_runnable.invoke({\"topic\": \"a new product launch for eco-friendly packaging\"})\n",
        "print(\"\\n--- 2. Invoking ChatOpenAI (LLM) ---\\n\")\n",
        "# What does .invoke() do for an LLM? It sends the prompt to the AI and gets a response.\n",
        "# The input to an LLM is typically the output of a prompt (a list of messages or a string).\n",
        "llm_response_message = llm_runnable.invoke(formatted_prompt.to_messages())\n",
        "print(f\"Input to LLM (type: {type(formatted_prompt.to_messages())}): A list of messages\")\n",
        "print(f\"\\nOutput of llm.invoke() (type: {type(llm_response_message)}):\")\n",
        "print(f\"  - ({llm_response_message.type}) Content: '{llm_response_message.content}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8fed5b64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fed5b64",
        "outputId": "e859f9ca-fe42-493f-90b8-89ba26fd660a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. Invoking StrOutputParser ---\n",
            "\n",
            "Input to parser (type: <class 'langchain_core.messages.ai.AIMessage'>): An AIMessage object\n",
            "\n",
            "Output of parser.invoke() (type: <class 'str'>): '\"Introducing Our Innovative Eco-Friendly Packaging Solutions: Join the Green Revolution!\"'\n"
          ]
        }
      ],
      "source": [
        "# C. Invoking the StrOutputParser\n",
        "print(\"\\n--- 3. Invoking StrOutputParser ---\")\n",
        "# What does .invoke() do for StrOutputParser? It takes an AI Message (or similar) and extracts the string content.\n",
        "# The input to the parser is typically the output of an LLM.\n",
        "parsed_string_output: str = parser_runnable.invoke(llm_response_message)\n",
        "print(f\"\\nInput to parser (type: {type(llm_response_message)}): An AIMessage object\")\n",
        "print(f\"\\nOutput of parser.invoke() (type: {type(parsed_string_output)}): '{parsed_string_output}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79eb7229",
      "metadata": {
        "id": "79eb7229"
      },
      "source": [
        "Now, the magic of LangChain Expression Language (LCEL)! The | (pipe) operator in LCEL is designed to work with Runnables. It leverages polymorphism.\n",
        "When you write:\n",
        "\n",
        "```python\n",
        "chain: Runnable = prompt_runnable | llm_runnable | parser_runnable\n",
        "```\n",
        "\n",
        "1. ```prompt_runnable | llm_runnable```:\n",
        "    - LCEL knows both are Runnables.\n",
        "    - It creates a sequence. When this sequence is invoked (e.g., with `chain.invoke(prompt_input)`):\n",
        "        - It first calls `prompt_runnable.invoke(prompt_input)`. The output is `formatted_prompt_value`.\n",
        "        - Then, it takes `formatted_prompt_value` and calls `llm_runnable.invoke(formatted_prompt_value.to_messages())`. The output is `llm_response_message`.\n",
        "        - The intermediate `RunnableSequence` (from `prompt_runnable | llm_runnable`) now has `llm_response_message` as its result.\n",
        "\n",
        "2. `(prompt_runnable | llm_runnable) | parser_runnable`:\n",
        "    - The `RunnableSequence` from step 1 is itself a Runnable.\n",
        "    - Its output (`llm_response_message`) is then piped to `parser_runnable`.\n",
        "    - `parser_runnable.invoke(llm_response_message)` is called, resulting in `parsed_string_output`.\n",
        "\n",
        "The | operator doesn't need to know the specific details of how each component implements invoke(). It only needs to trust that:\n",
        "\n",
        "- Each component has an `invoke()` method (satisfies the `Runnable` interface).\n",
        "- The output type of one component's `invoke()` is compatible with the input type of the next component's `invoke()`. (LangChain often handles common coercions, but type compatibility is important)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "acb0fc7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acb0fc7b",
        "outputId": "725d9587-25ca-4c04-ab4f-e26777ccbd71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Invoking the Full LCEL Chain ---\n",
            "\n",
            "Input to chain: {'topic': 'a new product launch for eco-friendly packaging'}\n",
            "\n",
            "Output of chain.invoke() (type: <class 'str'>): '\"Introducing Our Innovative Eco-Friendly Packaging Solutions: Join Us in Making a Sustainable Impact!\"'\n"
          ]
        }
      ],
      "source": [
        "# --- Composing and Invoking the Full LCEL Chain ---\n",
        "email_subject_chain = prompt_runnable | llm_runnable | parser_runnable\n",
        "\n",
        "print(\"\\n--- Invoking the Full LCEL Chain ---\")\n",
        "final_email_subject: str = email_subject_chain.invoke(prompt_input) # Same input as for the prompt alone\n",
        "\n",
        "print(f\"\\nInput to chain: {prompt_input}\")\n",
        "print(f\"\\nOutput of chain.invoke() (type: {type(final_email_subject)}): '{final_email_subject}'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCTR9cRg7tt9"
      },
      "id": "jCTR9cRg7tt9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agentic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}